spam
View(spam)
spam$capitalAveSq <- spam$capitalAve^2
View(spam)
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain,]
View(Wage)
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
head(dummies)
nsv <- nearZeroVar(training,saveMetrics=TRUE)
nsv
?nearZeroVar
library(splines)
bsBasis <- bs(training$age,df=3)
bsBasis
?bs
lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
predict(bsBasis,age=testing$age)
library(caret); library(kernlab); data(spam);
inTrain <- createDataPartition(y=spam$type,
p=0.75.list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[, -58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
library(caret); library(kernlab); data(spam);
inTrain <- createDataPartition(y=spam$type,
p=0.75.list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[, -58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
library(caret); library(kernlab); data(spam);
inTrain <- createDataPartition(y=spam$type,
p=0.75,list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[, -58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
names(spam[c(34,32)])
plot(spam[,34],spam[,32])
X <- 0.71*training$num415 + 0.71*training$num857
y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
prComp$rotation
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlob="PC1",ylab="PC2")
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
warnings()
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
warnings()
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit <- train(training$type ~.,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(training)
View(training)
View(training)
?cut2
library("Hmisc", lib.loc="~/R/win-library/3.1")
weightCut <- cut2(training$CompressiveStrength, g=8)
levels(weightCut) <- c(1:8)
qq <- qplot(CompressiveStrength, ., colour=weightcut)
qq + geom_smooth()
qq <- qplot(CompressiveStrength, ., colour=weightCut)
qq + geom_smooth()
qq <- qplot(CompressiveStrength, ., colour=weightCut, data=training)
qq + geom_smooth()
qplot(CompressiveStrength.colour=weightCut,data=training,geom="density")
View(concrete)
table(weightCut)
View(mixtures)
library(Hmisc)
cutStrength <- cut2(training$CompressiveStrength,g=4)
table(cutStrength)
ggplot(data = training, aes(y = index, x = cutStrength)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training, y = training$CompressiveStrength, plot = "pairs")
ggplot(data = training, aes(y = "density", x = cutStrength)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
ggplot(data = training, aes(y = cutStrength, x = "density")) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
ggplot(data = training, aes(y = "density", x = cutStrength)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram()
?prcomp
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(adData)
AlzheimerDisease
data(AlzheimerDisease)
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
predictors_IL <- predictors[, IL_str]
View(predictors_IL)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
c1
C1
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
data(iris); library(ggplot2)
names(iris)
View(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7,lisr=FALSE)
inTrain <- createDataPartition(y=iris$Species,
p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
View(iris)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
modFit
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE,
main="Classification Tree")
plot(modFit$finalModel,uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
?fancyRpartPlot
install.packages("rpart.plot")
libary(rpart.plot)
library(rpart.plot)
library(rpart.plot)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
install.packages("ElemStatLearn")
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
View(ozone)
head(ozone)
View(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
for (i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature - ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
View(ozone0)
for (i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
library("caret", lib.loc="~/R/win-library/3.1")
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
install.packages("party")
library(ElemStatLearn); library(caret);library(party)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone, temperature, col="lightgrey",pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7,list=FALSE)
View(iris)
View(inTrain)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species ~ .,data=training,method="rf",prox=TRUE)
library(caret)
modFit <- train(Species ~ .,data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); iris$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p <- geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
p
lot(Petal.Width, Petal.Length, col=Species,data=training)
plot(Petal.Width, Petal.Length, col=Species,data=training)
View(training)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p <- geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
p
View(irisP)
View(irisP)
library("ggplot2", lib.loc="~/R/win-library/3.1")
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p <- geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
modFit <- train(wage ~ .,method="glm",data=training,verbose=FALSE)
print(modFit)
modFit <- train(wage ~ .,method="gbm",data=training,verbose=FALSE)
modFit <- train(wage ~ .,method="gbm",data=training,verbose=FALSE)
print(modFit)
qplot(predict(modFit,testing),wage,data=testing)
data(iris);library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)
modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ .,data=training,method="nb")
warnings()
modnb = train(Species ~ .,data=training,method="nb")
warnings()
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal)
table(segmentationOriginal$Case)
train <- segmentationOriginal(segmentationOriginal$Case==Train)
train <- segment(segmentationOriginal$Case==Train)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
View(train)
View(segmentationOriginal)
View(segmentationOriginal)
table(segmentationOriginal$Class)
table(train$Class)
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=train)
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rpart.plot)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
predict(modFit,newdata=test)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
install.packages("tree")
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata)
View(olive)
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(testSA)
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
modFit
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
sqrt(sum((modFit$fitted-trainSA$chd)^2))
missClass(trainSA$chd, predict(modFit, type = "response"))
set.seed(13234)
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family = "binomial",
data = trainSA)
glm1
sqrt(sum((glm1$fitted-trainSA$chd)^2))
missClass(trainSA$chd, predict(glm1, type = "response"))
missClass(testSA$chd, predict(glm1, newdata = testSA, type = "response"))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
wel.train$y = factor(vowel.train$y)
vowel.test$y = factor(vowel.test$y)
vowel.train$y = factor(vowel.train$y)
vowel.test$y = factor(vowel.test$y)
str(vowel.train)
set.seed(33833)
modFit <- train(y ~ .,data=vowel.train,method="rf",prox=TRUE)
modFit
gbmImp <- varImp(modFit, scale = FALSE)
gbmImp
?varImp
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model = svm(CompressiveStrength ~ ., data = training)
model
pred = predict(model, testing)
RMSE = sqrt(sum((pred - testing$CompressiveStrength)^2))
predins = predict(model, training)
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
library(kernlab)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model = svm(CompressiveStrength ~ ., data = training)
model
?svm
install.packages("e1071")
set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
library(kernlab)
library(e1071)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model = svm(CompressiveStrength ~ ., data = training)
model
pred = predict(model, testing)
RMSE = sqrt(sum((pred - testing$CompressiveStrength)^2))
predins = predict(model, training)
RMSEins = sqrt(sum((predins - training$CompressiveStrength)^2))
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
install.packages("forecast")
set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
library(kernlab)
library(e1071)
library(forecast)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
date: 'r date()'
date: `r date()`
First Slide
First Slide
library(slidifiy)
library(slidify)
setwd("H:/My Documents/Coursera/Developing Data Products/Project/Data_Products_Project")
slidify("index.Rmd")
browseURL("index.html")
publish(user = "tonym472", repo = "tonym472.github.io")
publish(user = "tonym472", repo = "Developing-Data-Products")
publish(user = "tonym472", repo = "Developing-Data-Products")
publish(title = 'Data Prods', 'index.html', host = 'rpubs')
publish(user = "tonym472", repo = "DataProducts")
publish(user = "github.com", repo = "tonym472/DataProducts")
publish(repo = "tonym472/Developing-Data-Products")
publish(user = "tonym472", repo = "tonym472/DataProducts")
publish(user = "tonym472", repo = "tonym472/Developing-Data-Products")
publish(user = "tonym472", repo = "tonym472.github.io")
publish(user = "tonym472", repo = "Developing-Data-Products")
publish_github(user = "tonym472", repo = "Developing-Data-Products")
slidify("index.Rmd")
browseURL("index.html")
options(rpubs.upload.method = "internal")
options(rpubs.upload.method = "internal")
